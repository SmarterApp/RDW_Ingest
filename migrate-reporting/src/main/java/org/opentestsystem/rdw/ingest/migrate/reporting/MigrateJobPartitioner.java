package org.opentestsystem.rdw.ingest.migrate.reporting;

import org.opentestsystem.rdw.ingest.migrate.reporting.repository.ReportingMigrateRepository;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.batch.core.JobExecution;
import org.springframework.batch.core.JobParameters;
import org.springframework.batch.core.listener.JobExecutionListenerSupport;
import org.springframework.batch.core.partition.support.Partitioner;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.util.Map;

import static org.hibernate.validator.internal.util.CollectionHelper.newHashMap;
import static org.opentestsystem.rdw.ingest.migrate.reporting.JobParams.FirstImportId;
import static org.opentestsystem.rdw.ingest.migrate.reporting.JobParams.LastImportId;

/**
 * The {@link JobExecutionListenerSupport} to perform before/after job steps, as well as {@link Partitioner}
 * to partition job's parameter for parallel execution
 */
@Component
public class MigrateJobPartitioner extends JobExecutionListenerSupport implements Partitioner {

    private static final Logger logger = LoggerFactory.getLogger(MigrateJobPartitioner.class);
    private ReportingMigrateRepository reportingMigrateRepository;

    private static final String PARTITION_KEY = "partition";
    private long firstImportId;
    private long lastImportId;

    @Autowired
    void setReportingMigrateRepository(final ReportingMigrateRepository reportingMigrateRepository) {
        this.reportingMigrateRepository = reportingMigrateRepository;
    }

    @Override
    public void beforeJob(final JobExecution jobExecution) {
        final JobParameters jobParameters = jobExecution.getJobParameters();
        firstImportId = jobParameters.getLong(FirstImportId);
        lastImportId = jobParameters.getLong(LastImportId);
    }

    @Override
    public Map<String, ExecutionContext> partition(final int gridSize) {
        final long chunkSize = (lastImportId - firstImportId) / gridSize;

        long partitionFirstImportId = firstImportId;
        long partitionLastImportId = firstImportId + chunkSize;
        int i = 0;
        final Map<String, ExecutionContext> map = newHashMap(gridSize);
        while (partitionFirstImportId <= lastImportId) {
            final ExecutionContext ex = new ExecutionContext();
            ex.put(FirstImportId, partitionFirstImportId);
            ex.put(LastImportId, partitionLastImportId);
            map.put(PARTITION_KEY + i++, ex);

            partitionFirstImportId = partitionLastImportId + 1;
            partitionLastImportId = Math.min(partitionLastImportId + chunkSize, lastImportId);
        }
        return map;
    }
}
