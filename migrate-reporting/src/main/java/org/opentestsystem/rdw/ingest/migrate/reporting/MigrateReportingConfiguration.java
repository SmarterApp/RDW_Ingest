package org.opentestsystem.rdw.ingest.migrate.reporting;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.batch.core.Job;
import org.springframework.batch.core.JobExecutionListener;
import org.springframework.batch.core.Step;
import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;
import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;
import org.springframework.batch.core.launch.JobLauncher;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.cloud.stream.annotation.EnableBinding;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Import;

import org.opentestsystem.rdw.ingest.migrate.reporting.step.StagingToReportingStepsConfig;
import org.opentestsystem.rdw.migrate.common.MigrateJobExecutionListener;
import org.opentestsystem.rdw.migrate.common.MigrateJobProcessor;
import org.opentestsystem.rdw.migrate.common.TenantAwareMigrateJobHandler;
import org.opentestsystem.rdw.migrate.common.repository.MigrateRepository;
import org.opentestsystem.rdw.migrate.common.repository.WarehouseImportRepository;
import org.opentestsystem.rdw.multitenant.TenantIdResolver;

/**
 * MigrateReportingConfiguration.
 * Spring Batch configuration for the migrate to reporting {@link Job}.
 */
@Configuration
@EnableBatchProcessing
@Import(StagingToReportingStepsConfig.class)
@EnableBinding(MigrateJobProcessor.class)
public class MigrateReportingConfiguration {
    private static final Logger logger = LoggerFactory.getLogger(MigrateReportingConfiguration.class);

    final JobBuilderFactory jobBuilderFactory;

    @Autowired
    public MigrateReportingConfiguration(final JobBuilderFactory jobBuilderFactory) {

        this.jobBuilderFactory = jobBuilderFactory;
    }

    @Bean
    public JobExecutionListener listener(final MigrateRepository migrateRepository,
                                         final TenantIdResolver tenantIdResolver) {
        return new MigrateJobExecutionListener(migrateRepository, tenantIdResolver);
    }

    /**
     * This job migrates data from the 'warehouse' to 'reporting' databases.
     * <p>
     * The tables in the warehouse are divided into 'master' and 'children' tables based on their dependencies.
     * All master tables have the created/updated timestamps as well as a delete flag to indicate soft deletes.
     * <p>
     * There are also 'code' tables. They are special 'master's that do not have an explicit delete
     * or timestamps. Code tables are small in size and (if migrated) are moved all at once.
     * It is assumed that the data warehouse takes care of the data integrity in regards to the codes,
     * so it is safe to refresh all the codes.
     * <p>
     * The migration process is done in chunks defined by a range of of the created/updated timestamps.
     * <p>
     * The data is first moved into the staging tables and then migrated to the reporting data mart.
     * <p>
     * Staging tables do not enforce any data relationships and could be loaded in any order.
     * All records from the master tables within a batch timestamp range are moved to the staging tables.
     * The children tables are moved along with the corresponding masters. Note, that if a master is flagged as deleted, the
     * related children are <b>not</b> moved to the staging.
     * <p>
     * The migration from the staging to reporting is designed to preserve a referential integrity of the data in the reporting
     * data mart.
     * <ol>
     * <li>Since almost all the tables depend on the codes, the codes are inserted/updated first:
     * {@link StagingToReportingStepsConfig#upsertCodesStep()}</li>
     * <li>Next, the masters that are flagged as deleted are removed from the data mart along with the referenced data:
     * {@link StagingToReportingStepsConfig#deleteEntitiesStep()}</li>
     * <li>Then each master is processed along with its dependent children. The updates and inserts for the data are determined
     * by comparing staging to the reporting data. An 'update' to the master may trigger a 'delete' to a dependent table. For example, an
     * assessment had one item removed.</li>
     * <li>And finally, the code deletes are performed (if needed): {@link StagingToReportingStepsConfig#deleteCodesStep()}</li>
     * </ol>
     */
    @Bean
    public Job migrateReportingJob(final JobExecutionListener listener,
                                   final Step truncateBeforeStageStep,
                                   final Step stageCodesStep,
                                   final Step stageSubjectsStep,
                                   final Step stageOrganizationsStep,
                                   final Step stageEmbargoStep,
                                   final Step stageGroupsStep,
                                   final Step stagePackageStep,
                                   final Step stageNormsStep,
                                   final Step stageExamStep,
                                   final Step upsertCodesStep,
                                   final Step deleteEntitiesStep,
                                   final Step deleteCodesStep,
                                   final Step upsertSubjectsStep,
                                   final Step upsertOrganizationsStep,
                                   final Step updateEmbargoStep,
                                   final Step upsertAsmtsStep,
                                   final Step upsertNormsStep,
                                   final Step upsertStudentsAndGroupsStep,
                                   final Step upsertExamsStep,
                                   final Step truncateAfterStageStep) {
        try {
            return jobBuilderFactory.get("Migrate Reporting Job")
                    .preventRestart()
                    .listener(listener)
                    .start(truncateBeforeStageStep)
                    .next(stageCodesStep)
                    .next(stageSubjectsStep)
                    .next(stageOrganizationsStep)
                    .next(stageEmbargoStep)
                    .next(stageGroupsStep)
                    .next(stagePackageStep)
                    .next(stageNormsStep)
                    .next(stageExamStep)
                    .next(upsertCodesStep)
                    .next(deleteEntitiesStep)
                    // almost all of the data entities have dependency on the subject data
                    // the ingest should prevent updating the subject, if there are any other data that depends on it
                    .next(upsertSubjectsStep)
                    .next(upsertOrganizationsStep)
                    .next(updateEmbargoStep)
                    .next(upsertAsmtsStep)
                    .next(upsertNormsStep)
                    .next(upsertStudentsAndGroupsStep)
                    .next(upsertExamsStep)
                    .next(deleteCodesStep)
                    .next(truncateAfterStageStep)
                    .build();
        } catch (final Exception e) {
            logger.error("Failed to create migrate reporting job bean", e);
            return null;
        }
    }

    @Bean
    public TenantAwareMigrateJobHandler tenantAwareMigrateJobHandler(
            final MigrateRepository migrateRepository,
            final WarehouseImportRepository importRepository,
            final TenantIdResolver tenantIdResolver,
            final JobLauncher jobLauncher,
            final Job job,
            @Value("${migrate.batch.size}") final int batchSize,
            final MigrateJobProcessor migrateJobProcessor) {

        return new TenantAwareMigrateJobHandler(
                migrateRepository,
                importRepository,
                tenantIdResolver,
                jobLauncher,
                job,
                batchSize,
                migrateJobProcessor);
    }
}
