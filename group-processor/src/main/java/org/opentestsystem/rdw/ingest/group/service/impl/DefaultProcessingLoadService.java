package org.opentestsystem.rdw.ingest.group.service.impl;

import com.google.common.collect.ImmutableMap;
import org.opentestsystem.rdw.ingest.common.model.EntitySqls;
import org.opentestsystem.rdw.ingest.common.repository.SqlListExecutionRepository;
import org.opentestsystem.rdw.ingest.common.util.LocationStrategy;
import org.opentestsystem.rdw.ingest.common.util.SqlListBuilder;
import org.opentestsystem.rdw.ingest.group.configuration.GroupProcessingSqlConfiguration;
import org.opentestsystem.rdw.ingest.group.service.ProcessingLoadService;
import org.opentestsystem.rdw.archive.ArchiveService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.io.FileNotFoundException;
import java.text.MessageFormat;
import java.util.Enumeration;
import java.util.List;
import java.util.Map;
import java.util.Properties;


/**
 * DefaultProcessingLoadService - This service is responsible for loading the content of the
 * group batch csv file into the warehouse so that other processing may start.
 */
@Service
public class DefaultProcessingLoadService implements ProcessingLoadService {
    private static final Logger logger = LoggerFactory.getLogger(DefaultProcessingLoadService.class);
    private static final String s3Prefix = "s3://";
    private static final String localPrefix = "file://";
    static final String rawURI = "rawURI";

    @Value("${sql.load-csv}")
    public String rawSql;

    private final ArchiveService archiveService;

    private final SqlListExecutionRepository repository;
    private final GroupProcessingSqlConfiguration sqlConfiguration;

    @Autowired
    public DefaultProcessingLoadService(final ArchiveService archiveService,
                                        final SqlListExecutionRepository repository,
                                        final GroupProcessingSqlConfiguration sqlConfiguration) {
        this.archiveService = archiveService;
        this.repository = repository;
        this.sqlConfiguration = sqlConfiguration;
    }

    /**
     * Load the batch upload table with data located via the digest of the data
     * @param digest
     * @param batchId
     */
    @Override
    public void loadBatch(final String digest, final long batchId) {
        final String location = new LocationStrategy.GroupUploadContentLocationStrategy().location(digest);

        //Check if file already exists in ArchiveService
        if (!archiveService.exists(location)) {
            logger.warn("Unable to locate " + location + " via the archive service");
            throw new IllegalArgumentException("Unable to locate " + location + " via the archive service");
        }

        String sqlSource;
        Properties props = archiveService.readProperties(location);
        String uri = props.getProperty(rawURI);

        final Map<String, EntitySqls> entities = sqlConfiguration.getEntities();
        final SqlListBuilder sqlListBuilder = new SqlListBuilder(entities);

        // The sql statement is string parameterized to choose between S3 syntax and local file syntax
        // We want to get the sql statement, substitute in the right source, and then pass the sql statement
        //  off to the repository to be executed.


        if(uri.startsWith(s3Prefix)) {
            sqlSource = "FROM S3";
        }
        else if(uri.startsWith(localPrefix)){
            sqlSource = "LOCAL INFILE";
            uri = uri.substring(localPrefix.length());
        }
        else {
            throw new IllegalArgumentException("Invalid URI for location " + location);
        }

        List<String> sql = sqlListBuilder.addNext(String.format(rawSql, sqlSource)).build();
        repository.execute(sql, ImmutableMap.of("uri", uri, "batch_id", batchId));
    }
}
