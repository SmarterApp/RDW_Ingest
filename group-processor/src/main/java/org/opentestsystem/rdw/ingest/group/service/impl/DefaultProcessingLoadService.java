package org.opentestsystem.rdw.ingest.group.service.impl;

import com.google.common.collect.ImmutableMap;
import org.opentestsystem.rdw.ingest.common.model.EntitySqls;
import org.opentestsystem.rdw.ingest.common.repository.SqlListExecutionRepository;
import org.opentestsystem.rdw.ingest.common.util.LocationStrategy;
import org.opentestsystem.rdw.ingest.common.util.SqlListBuilder;
import org.opentestsystem.rdw.ingest.group.configuration.GroupProcessingSqlConfiguration;
import org.opentestsystem.rdw.ingest.group.service.ProcessingLoadService;
import org.opentestsystem.rdw.archive.ArchiveService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.text.MessageFormat;
import java.util.Enumeration;
import java.util.List;
import java.util.Map;
import java.util.Properties;


/**
 * DefaultProcessingLoadService - This service is responsible for loading the content of the
 * group batch csv file into the warehouse so that other processing may start.
 */
@Service
public class DefaultProcessingLoadService implements ProcessingLoadService {
    private static final Logger logger = LoggerFactory.getLogger(DefaultProcessingLoadService.class);
    private final String s3Prefix = "s3://";
    private final String localPrefix = "file://";
    private final String rawURI = "rawURI";

    private final ArchiveService archiveService;

    private final SqlListExecutionRepository repository;
    private final GroupProcessingSqlConfiguration sqlConfiguration;

    @Autowired
    public DefaultProcessingLoadService(final ArchiveService archiveService,
                                        final SqlListExecutionRepository repository,
                                        final GroupProcessingSqlConfiguration sqlConfiguration) {
        this.archiveService = archiveService;
        this.repository = repository;
        this.sqlConfiguration = sqlConfiguration;
    }

    /**
     * Load the batch upload table with data located via the digest of the data
     * @param digest
     * @param batchId
     */
    @Override
    public void loadBatch(final String digest, final long batchId) {
        final String location = new LocationStrategy.GroupUploadContentLocationStrategy().location(digest);
        String sqlSource;
        //Check if file already exists in ArchiveService
        if (archiveService.exists(location)) {
            Properties props = archiveService.readProperties(location);
            List<String> sql;
            String rawSql;
            String uri = props.getProperty(rawURI);
            logger.info(uri);
            final Map<String, EntitySqls> entities = sqlConfiguration.getEntities();
            final Map<String, String> entitySqls = entities.get("load").getSql();
            final SqlListBuilder sqlListBuilder = new SqlListBuilder(entities);
            if (!entitySqls.containsKey("fromFile")) throw new IllegalArgumentException("unknown sql name [fromFile] for entity [load], check sql configuration");

            rawSql = (String)entitySqls.get("fromFile");

            if(uri.startsWith(s3Prefix)) {
                sqlSource = "FROM S3";
            }
            else {
                sqlSource = "LOCAL INFILE";
                uri = uri.substring(localPrefix.length());
            }
            rawSql = String.format(rawSql, sqlSource);
            sql = sqlListBuilder.addNext(rawSql).build();

            repository.execute(sql, ImmutableMap.of("uri", uri,
                    "batch_id", batchId));
        }

    }
}
